# ğŸš€ High-Performance CSV Processing (5GB+)

![Python](https://img.shields.io/badge/Python-3.x-blue)
![Pandas](https://img.shields.io/badge/Pandas-Data%20Analysis-orange)
![Dask](https://img.shields.io/badge/Dask-Parallel%20Computing-green)
![Status](https://img.shields.io/badge/Project-Academic-blueviolet)

---

## ğŸ“Œ Project Overview

This project demonstrates efficient techniques for processing very large CSV files (~5GB) in Python without causing memory overflow.

The experiment compares different Big Data approaches in terms of:

- â± Execution time  
- ğŸ’¾ Storage efficiency  
- ğŸ§  Memory management  

### Dataset Used

- File name: `ACI-IoT-2023-Payload.csv`
- File size: **5276.13 MB (~5.2 GB)**

> âš  Dataset not included in this repository due to its large size.

---

## ğŸ— Project Structure

```
High-Performance-CSV-Processing/
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ pandas_chunks.py
â”‚   â”œâ”€â”€ dask_reader.py
â”‚   â””â”€â”€ compressor.py
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ timer.py
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Technologies Used

- Python 3.x  
- Pandas  
- Dask  
- Matplotlib  
- gzip  

---

## ğŸ”¬ Methods Implemented

### 1ï¸âƒ£ Pandas Chunking

```python
pd.read_csv(file, chunksize=100000)
```

âœ” Memory efficient  
âœ” Stable  
âŒ Sequential processing  

**Execution Time:** `40.63 seconds`

---

### 2ï¸âƒ£ Dask (Parallel Processing)

```python
dd.read_csv(file)
```

âœ” Fastest method  
âœ” Multi-core processing  
âœ” Designed for large datasets  

**Execution Time:** `25.31 seconds`

---

### 3ï¸âƒ£ File Compression (gzip)

- Original Size: **5276.13 MB**
- Compressed Size: **2968.57 MB**
- Storage Reduction: **~44%**

âœ” Reduces disk usage  
âŒ High processing time  

**Execution Time:** `295.48 seconds`

---

## ğŸ“Š Experimental Results

| Method          | Execution Time (sec) | File Size (MB) |
|----------------|----------------------|----------------|
| Pandas Chunks | 40.63               | 5276.13       |
| Dask          | 25.31               | 5276.13       |
| Compression   | 295.48              | 2968.57       |

---

## ğŸ“ˆ Performance Analysis

- ğŸ¥‡ **Fastest Approach:** Dask  
- ğŸ’¾ **Best Storage Optimization:** Compression (~44% reduction)  
- âš– **Balanced Approach:** Pandas Chunking  

Dask outperformed Pandas due to parallel computation.  
Compression significantly reduced storage size but required much more processing time.

---

## ğŸ§  Key Takeaways

- Large CSV files cannot be safely loaded entirely into memory.
- Chunking improves memory control.
- Parallel processing significantly improves execution speed.
- Compression trades CPU time for storage efficiency.

---

## â–¶ï¸ How to Run

Install dependencies:

```bash
pip install -r requirements.txt
```

Run the project:

```bash
python main.py
```

---

## ğŸ¯ Conclusion

This project demonstrates practical and scalable techniques for handling large-scale datasets efficiently in Python.

- Use **Dask** for performance-critical workloads.
- Use **Compression** for storage optimization.
- Use **Pandas chunking** for controlled memory usage.
